{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several steps:\n",
    "Let W, H denote your chosen input image size. If inputs from camera are too large you can downscale them to a reasonable value (perform parameter estimate).\n",
    "1. Preprocess images. Output is tuple (X_train, y_train, X_val, y_val, x_test, y_test) of examples. Data should be 60% training, 20% validation, and 20% test. Each element of the tuple is of the form (N, 3, H, W). 3 is the number of YUV color channels (df to convert)\n",
    "2. Build model. Use VGG net for conv net with a couple dense layers of top.\n",
    "3. Train model, validate\n",
    "4. Unfreeze last layers of convnet head and fine tune\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "First, we convert the video to a set of images using opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_steering_angle(framenum):\n",
    "    return np.random.rand() * 5\n",
    "\n",
    "def generate_pictures_from_video(video_name):\n",
    "    cap = cv2.VideoCapture(video_name)\n",
    "    framenum = 1\n",
    "    if 'data' not in os.listdir():\n",
    "        os.mkdir('data')\n",
    "        \n",
    "    ret, frame = cap.read()\n",
    "    while ret:\n",
    "        # get steering angle\n",
    "        angle = get_steering_angle(framenum)\n",
    "        imgname = 'data/frame_%d_angle_%.3f.jpg' % (framenum, angle)\n",
    "        cv2.imwrite(imgname, frame)\n",
    "        \n",
    "        framenum += 1\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "VIDEO_NAME = \"video01.avi\"\n",
    "generate_pictures_from_video(VIDEO_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy parameters\n",
    "N = 1000\n",
    "H = W = 120\n",
    "\n",
    "# Let N = total number of images\n",
    "# Return (data, labels), where:\n",
    "# data is numpy array of shape (N, 3, H, W). \n",
    "#     Each data[i, :, :] represents an RGB image\n",
    "# labels is numpy array of shape (N, 1)\n",
    "#     labels[i] represents a steering angle, float32\n",
    "def rgb2yuv(data):\n",
    "    for i in range(N):\n",
    "        img = data[i, :, :]\n",
    "        img = np.transpose(img, axes=(1, 2, 0)).astype(np.float32)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        img = np.transpose(img, axes=(2, 0, 1))\n",
    "        data[i, :, :] = img\n",
    "    return data\n",
    " \n",
    "    \n",
    "# generate plaecholder random images for now\n",
    "def load_data():\n",
    "    # TODO: Set N = number of examples\n",
    "#     data = np.random.randint(255, size=(N, 3, H, W))\n",
    "    \n",
    "    labels = np.random.rand(N)\n",
    "    \n",
    "    data = rgb2yuv(data)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def get_data():\n",
    "    data, labels = load_data()\n",
    "    \n",
    "    train_size = int(0.6 * N)\n",
    "    val_size = int(0.2 * N)\n",
    "    test_size = int(0.2 * N)\n",
    "    \n",
    "    X_train = data[:train_size]\n",
    "    X_val = data[train_size : train_size + val_size]\n",
    "    X_test = data[train_size + val_size:]\n",
    "    \n",
    "    y_train = labels[:train_size]\n",
    "    y_val = data[train_size : train_size + val_size]\n",
    "    y_test = data[train_size + val_size:]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import *\n",
    "def make_model():\n",
    "    model = models.Sequential()\n",
    "    conv_base = applications.VGG16(weights='imagenet',\n",
    "                                  include_top=False,\n",
    "                                  input_shape=(H, W, 3))\n",
    "    conv_base.trainable = False\n",
    "    # TODO: cross validation pipeline\n",
    "    model.add(conv_base)\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(loss='mse',\n",
    "                 optimizer=optimizers.Adam(lr=1e-2),\n",
    "                 metrics=['acc'])\n",
    "    # TODO: Add model training here\n",
    "#     history = model.fit()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "def make_generators(X_train, y_train, X_val=None, y_val=None):\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                      rotation_range=40,\n",
    "                                      width_shift_range=.2,\n",
    "                                      height_shift_range=.2,\n",
    "                                      shear_range=.2,\n",
    "                                      zoom_range=.2,\n",
    "                                      horizontal_flip=True,\n",
    "                                      vertical_flip=True,\n",
    "                                      fill_mode='nearest')\n",
    "    train_generator = train_datagen.flow(X_train,\n",
    "                                         y_train,\n",
    "                                         batch_size=32)\n",
    "    if X_val is None:\n",
    "        return train_generator\n",
    "    \n",
    "    val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    val_generator = val_datagen.flow(X_val,\n",
    "                                    y_val,\n",
    "                                    batch_size=32)\n",
    "    return train_generator, val_generator    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = get_data()\n",
    "train_generator, val_generator = make_generators(X_train, y_train, X_val, y_val)\n",
    "model = make_model()\n",
    "\n",
    "# model.fit(train_generator,\n",
    "#          epochs=5,\n",
    "#          validation_data=val_generator,\n",
    "#          verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 3, 120, 120)\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
